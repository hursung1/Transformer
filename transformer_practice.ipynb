{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "참고자료\n",
    "* https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "* https://tutorials.pytorch.kr/beginner/torchtext_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLOXxSceqMkc"
   },
   "source": [
    "# Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5qzEJx9qMkd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sp4_a5RlqMkh"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpVNNLVSqMki"
   },
   "source": [
    "변환 모델은 대부분 인코더-디코더 구조를 가지고 있다 [(cite)](https://arxiv.org/abs/1409.0473). 인코더는 입력 문장의 representation $(x_1, ..., x_n)$ 을 연속적인 representation $\\mathbf{z} = (z_1, ..., z_n)$ 의 시퀀스로 변환한다. 주어진 $\\mathbf{z}$에 대하여, 디코더는 출력 시퀀스 $(y_1,...,y_m)$ 를 생성한다. 각 step에서 모델은 하나의 출력을 다음 출력 단계의 입력으로 넣어주는 auto-regression[(cite)](https://arxiv.org/abs/1308.0850)을 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlfiWGpGqMkj"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module): # 일반적인 인코더-디코더 모델\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # src_mask는 source 문장의 <pad>에 대해 masking하기 위함\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EShtEOhHqMkk"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module): # 선형변환 후 softmax 함수를 통한 확률 분포 생성\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRODv6xtqMko"
   },
   "source": [
    "## Encoder and Decoder Stacks   \n",
    "트랜스포머 모델은 attention과 fully connected layer들을 여러개 쌓아(stack) 상기한 인코더-디코더 구조를 이룬다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1JE6r_J7wvH"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "\n",
    "인코더는 $N=6$개의 동일한 레이어를 쌓음으로써 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb-tgh0uqMkp"
   },
   "outputs": [],
   "source": [
    "def clones(module, N): # N개의 동일한 module을 생성하여 하나의 모듈로 만듦\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TT9gMbfgqMkp"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): # N개 레이어를 갖는 인코더\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qq9js9IqMkt"
   },
   "source": [
    "인코더의 각 레이어는 두 개의 서브레이어(sublayer)를 갖는다. 첫째는 multi-head self-attention, 둘째는 position-wise feed-forward network이다. 여기서 feed-forward network는 fully connected layer와 같다고 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPkAAMbtqMkt"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module): # 인코더는 self attention + feed forward layer로 구성\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization은 batch 내의 특정 feature에 대한 normalization을 하지만, Layer normalization은 batch 내의 각 데이터 별로 normalization을 수행한다. Normalization은 다음의 식과 같이 계산된다.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHWeFUwmqMkr"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # Layer Normalization\n",
    "        # Refer to above equation\n",
    "        ln = (\"...fill here...\")\n",
    "        ln = self.a_2 * ln + self.b_2\n",
    "        return ln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC2uw6ZNqMkr"
   },
   "source": [
    "Residual connection [(cite)](https://arxiv.org/abs/1512.03385)을 적용한 후 layer normalization [(cite)](https://arxiv.org/abs/1607.06450)을 수행한다. 이는 2개의 각 sublayer에 대해 모두 적용한다. 즉, 각 sublayer의 출력은 $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$와 같다. $\\mathrm{Sublayer}(x)$는 attention과 feed-forward network와 같다. 각 sublayer의 출력에 dropout [(cite)](http://jmlr.org/papers/v15/srivastava14a.html)을 적용할 수 있다.\n",
    "\n",
    "Residual connection을 이용하기 위해 임베딩 레이어를 포함한 모델의 모든 sublayer들은 $d_{\\text{model}}=512$ 차원의 출력을 생성한다. Residual connection을 통해 하위 레이어의 데이터가 연산을 거치면서 손실되는 것을 방지할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aUNOCC1qMks"
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module): # layer normalization 후 residual connection 수행\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        residual connection을 적용\n",
    "        코드 작성 편의를 위해 normalization 이후 residual을 적용하였음.\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKpzOXILqMkt"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "인코더와 동일하게, 디코더 또한 동일한 $N=6$개의 레이어를 쌓음으로써 구성된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHnVYa_EqMku"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):# masking이 추가된 N개 레이어를 갖는 decoder\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxmE1BUgqMkw"
   },
   "source": [
    "인코더에서는 서브레이어가 두 개였는데, 디코더에서는 세 개로 구성된다. 첫째로 <b>masked multi-head self attention</b>을, 그 다음으로 인코더의 출력에 대한 <b>multi-head attention</b>을 수행하는 layer로 구성된다. 마지막에는 <b>feed-forward network layer</b>로 구성되어 있다. 인코더와 마찬가지로 각 서브레이어의 출력과 residual connection 후 Layer Normalization을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HKtCsu-qMkw"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):# self attention, source attention, feed forward로 구성\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory # 인코더의 output\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) # masking을 포함한 self-attention\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) # 인코더 output과의 어텐션\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어텐션\n",
    "\n",
    "어텐션은 query, key, value에 대한 함수라고 볼 수 있다. query, key, value는 물론, 어텐션의 결과 또한 벡터이다. 결과는 value의 weighted sum으로 계산된다. 그 weight는 각 value에 할당되는데, 이 값은 query와, 해당하는 key 간의 compatibility 함수를 통해 계산된다.\n",
    "\n",
    "여기서 사용할 어텐션은 \"scaled dot-product attention\"이다. query 및 key는 $d_k$ 차원, value는 $d_v$ 차원으로 이루어진다. query와 모든 key간 dot product를 수행한 후, 그 결과를 $\\sqrt{d_k}$로 나눈다. 이후 softmax 함수를 사용하여 value에 대한 weight를 구한다.\n",
    "\n",
    "실제로는 모든 query에 대해 동시에 계산한다. 여기서 모든 query를 하나의 행렬로 나타낸 것을 $Q$라고 하자. key와 value들 또한 하나의 행렬($K$ 및 $V$)로 나타낸다. 어텐션 연산 결과 행렬을 다음을 통해 구할 수 있다:\n",
    "\n",
    "$$                                                                         \n",
    "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V               \n",
    "$$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbctuAB5qMk7"
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Scaled-Dot Product Attention을 수행\n",
    "    Query: ((head의 수), query 벡터의 수, d_k)\n",
    "    Key:   ((head의 수), key 벡터의 수, d_k)\n",
    "    Value: ((head의 수), value 벡터의 수, d_v)\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # Query와 Key 간 연산 (dot product)\n",
    "    # 1) 제곱근 구하는 함수: math.sqrt(x)\n",
    "    # 2) 행렬의 전치: x.transpose(axis1, axis2)\n",
    "    # 3) 행렬간 곱: torch.matmul(X, Y)\n",
    "    scores = (\"...fill here...\")\n",
    "\n",
    "    if mask is not None: # masking 수행\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Softmax\n",
    "    # softmax 구하는 함수: F.softmax()\n",
    "    p_attn = (\"...fill here...\")\n",
    "    \n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQDWYvOmqMk7"
   },
   "source": [
    "가장 많이 사용되는 어텐션 함수는 additive attention [(cite)](https://arxiv.org/abs/1409.0473)과 dot-product (multiplicative) attention이다. scaling factor가 $\\frac{1}{\\sqrt{d_k}}$로 다르다는 점만 빼면 여기서 사용하는 어텐션은 dot-product attention과 동일하다. 이론적으로, 계산 복잡도라는 측면에서 두 방식은 동일하지만 dot-product attention이 더 빠르고, 메모리 공간도 덜 사용한다. 고도로 최적화된 행렬곱 코드를 기반으로 구현할 수 있기 때문이다.\n",
    "\n",
    "$d_k$가 작으면 두 방식은 비슷하게 동작한다. 그러나 $d_k$가 큰 경우 scaling 없이는 additive attention이 더 나은 성능을 보인다[(cite)](https://arxiv.org/abs/1703.03906). 여기서, 원저자들은 $d_k$가 커지면, dot product의 결과 값이 매우 커지고, 따라서 softmax 함수가 아주 작은 미분(gradient)값을 가질 수 있다고 생각하였다. 이를 타개하기 위해 $\\frac{1}{\\sqrt{d_k}}$ 값으로 scaling한다.\n",
    "\n",
    "multi-head 어텐션을 통해, 모델이 서로 다른 representation 부공간(subspace)에서 얻을 수 있는 정보에 동시에 attend 할 수 있다. 즉, 한 문장의 여러 부분에 대해 어텐션을 할 수 있다는 것이다. single-head 어텐션에서는 평균을 취하기 때문에 그렇게 하지 못한다.\n",
    "\n",
    "$$    \n",
    "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O    \\\\                                           \n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)                                \n",
    "$$                                                                                                                 \n",
    "\n",
    "파라미터 행렬 $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$, $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$ 을 사용하여 각 부공간에 사영(projection)된다.\n",
    "\n",
    "본 논문에서는 $h=8$로 지정하여, $d_k=d_v=d_{\\text{model}}/h=64$ 값을 갖도록 하였다. 차원이 축소되었기 때문에 전체적으로 보면 single-head 어텐션과 비슷한 연산량을 갖는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JecAcEiPqMk8"
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"모델의 크기 및 head의 수(h)를 입력받음\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # 모든 head들에 대해 동일한 mask를 적용.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) batch내의 모든 데이터에 대해 선형 변환 => h x d_k \n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                            for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) batch내 사영된 모든 데이터들에 대해 어텐션 수행\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concatenation\" 후, 선형 변환\n",
    "        # contiguous 함수는 텐서의 형태 변환(view 함수)을 위해 사용\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "5hBB0XvSqMkz"
   },
   "source": [
    "### Masking\n",
    "디코더에서, self attention 부분에 masking이 추가된다. 이는 각 위치에서, 이후 위치에 대해 어텐션을 하지 못하도록 한다. Masking을 통해 $i$번째 위치에서 예측할 때, 오직 알고 있는, 즉 $i$ 이전의 결과에 대해서만 의존하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZmtzjfzqMk0"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"각 위치의 이후 위치에 mask를 적용\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    mask_torch = (torch.from_numpy(subsequent_mask) == 0)\n",
    "    return mask_torch.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPWjOT2TqMk0"
   },
   "source": [
    "아래에서 보여주는 그래프는 각 target의 word(row)가 의존할 수 있는 위치(column)를 나타낸다. 학습 과정에서 각 word들이 이후의 단어에 대해 attention을 하지 못하도록 차단한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 2449,
     "status": "ok",
     "timestamp": 1609045173845,
     "user": {
      "displayName": "Seong-Hwan Heo",
      "photoUrl": "https://lh5.googleusercontent.com/-0AkRh-q73zM/AAAAAAAAAAI/AAAAAAAAL_I/ftyesh65uHs/s64/photo.jpg",
      "userId": "17008924282569558701"
     },
     "user_tz": -540
    },
    "id": "iPi7Vhz5qMk4",
    "outputId": "10ad6781-4059-4e14-cca6-1ce0f880f8fb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(10).cpu()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsOCJD5gqMk9"
   },
   "source": [
    "### 트랜스포머 모델에 적용한 어텐션\n",
    "\n",
    "트랜스포머 모델에 적용된 어텐션을 다음과 같이 정리할 수 있다:\n",
    "\n",
    "1) 인코더-디코더 어텐션 레이어에서, query는 직전의 디코더 레이어의 출력값, key와 value는 인코더의 출력값을 사용한다. 이는 디코더의 각 모든 위치에서 입력 문장의 모든 위치에 대해 attend 할 수 있도록 한다. seq2seq 방식의 인코더-디코더를 흉내낼 수 있다.\n",
    "\n",
    "2) 인코더의 self-attention은 query, key, value가 모두 입력 벡터로 같다. 정확히는 직전 단계 인코더의 출력값이라고 볼 수 있다. 인코더의 각 위치에서 직전 인코더 출력값의 모든 위치에 대해 attend 할 수 있다.\n",
    "\n",
    "3) 디코더에서도 2)와 비슷하다. 다만, leftward information(문장의 오른쪽에서 왼쪽으로 전달되는 정보)를 차단하기 위해, 어텐션 내부에서 softmax함수의 입력값에 mask를 적용한다($-\\infty$로 덧씌움). 디코더에서 leftward information을 차단하는 이유는 auto-regressive 특성을 보존하기 위함이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_utWP2CqMk9"
   },
   "source": [
    "## Position-wise Feed-Forward Networks\n",
    "\n",
    "어텐션 수행 후 fully connected layer를 통과한다. 이는 각 위치에 대해 독립적으로, 그러나 똑같이 적용된다. 선형변환 후, 활성 함수로 ReLU를 적용하고 나서, 다시 선형 변환을 수행한다.\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n",
    "\n",
    "각 위치에 대해 선형 변환을 수행할 때 각 레이어는 서로 다른 parameter를 갖는다. kernel size = 1인 두 개의 convolution이 있다고 생각해도 될 것이다. 입력 및 출력의 차원은 $d_{\\text{model}}=512$이고, 내부 레이어의 차원은 $d_{ff}=2048$이다. 모든 위치에서 모두 같은 weight parameter를 공유하지만, 각 레이어에 따라서 달라진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TquXYwNnqMk9"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed forward network\n",
    "    선형 레이어 2개와, ReLU 함수로 구성\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lk8MDcfFqMk-"
   },
   "source": [
    "## 임베딩 및 Softmax\n",
    "\n",
    "여타 다른 시퀀스 변환모델과 같이, 입/출력되는 단어들을 $d_{\\text{model}}$차원의 벡터로 바꾸기 위해 미리 학습된 임베딩을 사용한다. 또한 다음 단어의 예측 확률을 구하기 위해 선형 변환 및 softmax 함수를 사용한다. 여기서, 임베딩 레이어 및 softmax 직전 선형 변환 레이어에서 동일한 weight matrix를 갖는다. 임베딩 레이어의 weight에 $\\sqrt{d_{\\text{model}}}$를 곱해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSDofWZVqMk-"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kerU_o22qMk-"
   },
   "source": [
    "## Positional Encoding\n",
    "트랜스포머 모델의 특징으로, 순환(recurrence) 구조도 없고 convolution 구조도 없다. 문장 내 단어들의 순서 정보를 사용할 수 있도록, 각 단어들의 상대적/절대적인 위치 정보를 추가해 줄 필요가 있다. 이를 위해 각 단어의 \"positional encoding\" 정보를 구한 다음 임베딩에 추가하여, 인코더 및 디코더의 입력으로 해줄 수 있다. 물론, positional encoding 또한 벡터이고, $d_{\\text{model}}$ 차원을 갖는다. 여러가지 방법이 있겠지만, 여기서는 서로 다른 주기를 갖는 삼각함수(사인 및 코사인)를 사용하여 구하도록 한다. \n",
    "\n",
    "$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
    "\n",
    "$$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})$$             \n",
    "\n",
    "$pos$는 단어의 위치, $i$는 차원을 의미한다. 즉, positional encoding의 각 차원은 하나의 사인 곡선에 해당한다. 파장은 $2\\pi$에서 $10000 \\cdot 2\\pi$까지 변하게 된다. 본 논문의 저자들은 이러한 함수를 사용함으로써 모델이 상대적인 위치에 따라 더 나은 attention 결과를 보일 수 있다는 가설을 세웠다. 고정된 offset $k$에 대해, $PE_{pos+k}$는 $PE_{pos}$의 선형 함수로 나타내어 질 수 있기 때문이다. 추가로, dropout을 적용할 수도 있다. 기본적으로는 $P_{drop}=0.1$를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zP9norLFqMk-"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # 로그를 사용하여 한번에 positional encoding을 계산\n",
    "        pe = torch.zeros(max_len, d_model, requires_grad=False)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1) # 위 수식에서 'pos'에 해당, shape=(max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model)) # 위 수식에서 1/10000^(2i/d_model)에 해당, shape=(d_model/2, )\n",
    "        # Positional Encoding\n",
    "        # 차원이 홀수인 경우: cos 함수 사용\n",
    "        # 차원이 짝수인 경우: sin 함수 사용\n",
    "        pe[:, 0::2] = (\"...fill here...\")\n",
    "        pe[:, 1::2] = (\"...fill here...\")\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 통해 $i$값에 따른 positional encoding이 어떻게 변화하는 지 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 2986,
     "status": "ok",
     "timestamp": 1609045174438,
     "user": {
      "displayName": "Seong-Hwan Heo",
      "photoUrl": "https://lh5.googleusercontent.com/-0AkRh-q73zM/AAAAAAAAAAI/AAAAAAAAL_I/ftyesh65uHs/s64/photo.jpg",
      "userId": "17008924282569558701"
     },
     "user_tz": -540
    },
    "id": "hhfnjXiJqMlD",
    "outputId": "0f2f5227-5ac1-4642-b89d-2ce13acd4cab"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(torch.zeros(1, 100, 20, requires_grad=True))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQutIEv_qMlE"
   },
   "source": [
    "본 논문의 저자들은 positional encoding을 학습하도록 하는 모델[(cite)](https://arxiv.org/pdf/1705.03122.pdf)도 실험을 진행하였는데, 결과적으로 사인곡선을 사용하는 방법이나 학습하도록 하는 방법 모두 같은 결과를 보였다고 한다. 여기서 사인곡선을 이용한 것은 모델이 학습하는 동안 다루었던 것보다 훨씬 더 긴 문장을 마주하였을 때에도 추론을 잘 할 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FUwm8PnqMlE"
   },
   "source": [
    "## Full Model\n",
    "\n",
    "다음 함수는 hyperparameter 및 각 구성요소들을 정의하여, 하나의 완전한 트랜스포머 모델을 리턴해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZrVK9JyqMlH"
   },
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), # 인코더\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),                   # 디코더\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)), # source 문장의 임베딩 레이어\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), # target 문장의 임베딩 레이어\n",
    "        Generator(d_model, tgt_vocab))                              # 문장 생성기\n",
    "    \n",
    "    # 모델 파라미터 초기화\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "        p = p.to(device)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlJZriFqMlI"
   },
   "source": [
    "# Training\n",
    "\n",
    "여기서부터는 트랜스포머 모델의 학습 과정에 필요한 함수 및 클래스를 정의할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sqVvL01qMlI"
   },
   "source": [
    "## Batches and Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSqJ_CWGqMlI"
   },
   "source": [
    "일반적인 인코더-디코더 모델을 학습하기 위해 필요한 몇 가지 유용한 코드를 정의할 것이다. 먼저 학습을 위해 source 및 target 문장들을 갖고 있는 batch 클래스 및 masking을 수행하는 함수를 함께 정의할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SE_aK_glqMlJ"
   },
   "outputs": [],
   "source": [
    "class Batch: # 학습하는 동안 필요한 데이터 및 mask를 갖고 있는 클래스\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2).to(device)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        # padding된 영역 및 이후 위치에 대한 mask를 생성\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
    "        return tgt_mask.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpdTXo2BqMlJ"
   },
   "source": [
    "## Training Loop\n",
    "다음으로 학습 및 scoring 함수를 정의하여 loss 값을 지속적으로 확인하고자 한다. parameter를 갱신하는 loss 계산함수를 함수의 인자로 전달한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K53tllwqMlK"
   },
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # GPU 텐서로 변환\n",
    "        src = batch.src.to(device)\n",
    "        trg = batch.trg.to(device)\n",
    "        src_mask = batch.src_mask.to(device)\n",
    "        trg_mask = batch.trg_mask.to(device)\n",
    "        \n",
    "        out = model.forward(src, trg, src_mask, trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR5jkfjXqMlO"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "$\\beta_1=0.9$, $\\beta_2=0.98$, $\\epsilon=10^{-9}$로 설정된 Adam optimizer를 사용한다. 학습이 진행되면서 learning rate를 다음 공식에 따라 변화시킨다.\n",
    "\n",
    "$$                                                                                                                                                                                                                                                                                         \n",
    "lrate = d_{\\text{model}}^{-0.5} \\cdot                                                                                                                                                                                                                                                                                                \n",
    "  \\min({step\\_num}^{-0.5},                                                                                                                                                                                                                                                                                                  \n",
    "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})                                                                                                                                                                                                                                                                               \n",
    "$$\n",
    "\n",
    "처음 $warmup\\_steps$ 횟수 동안 learning rate가 선형적으로 증가한다. 이후 점차 일정 비율(step수의 -1/2 승)로 감소한다. 여기서는 $warmup\\_steps=4000$으로 설정하였다.                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rxZBRKmqMlP"
   },
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    def __init__(self, model_size, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self): # parameter 및 learning rate를 업데이트\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None): # lrate를 계산\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNPnX6mbqMlP"
   },
   "source": [
    "다음은 learning rate가 어떻게 변화하는 지 보여주는 그래프이다. 그래프의 범례를 보면 x:y 형태인데, x는 $step\\_num$, y는 $warmup\\_steps$ 이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 3275,
     "status": "ok",
     "timestamp": 1609045174778,
     "user": {
      "displayName": "Seong-Hwan Heo",
      "photoUrl": "https://lh5.googleusercontent.com/-0AkRh-q73zM/AAAAAAAAAAI/AAAAAAAAL_I/ftyesh65uHs/s64/photo.jpg",
      "userId": "17008924282569558701"
     },
     "user_tz": -540
    },
    "id": "9Z6xdfVoqMlP",
    "outputId": "6f0b7406-fed8-4c55-9f0a-8d59d025e82a"
   },
   "outputs": [],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [NoamOpt(512, 4000, None), \n",
    "        NoamOpt(512, 8000, None),\n",
    "        NoamOpt(256, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZOM_9RQqMlS"
   },
   "source": [
    "## Loss 계산 함수\n",
    "loss 계산 후, normalization 된 값을 되돌려준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Psbm6szqMlS"
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.data.item() * norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJJKT1s9qMlT"
   },
   "source": [
    "# A Real World Example\n",
    "\n",
    "이제 torchtext를 통해 데이터를 불러와서 학습을 시켜보자. Multi30k 데이터셋을 사용하고자 한다. 평균적으로 13개의 단어로 구성된 30000개의 영어 및 독어 문장을 불러와, 이를 학습하여 영-독 번역기를 학습한다. Spacy를 Tokenizer로서 사용한다.\n",
    "\n",
    "다음 세 명령어를 터미널에 입력하여, torchtext및 spacy를 설치하고, spacy의 영어 및 독어 tokenizer를 다운로드한다. \n",
    "```bash\n",
    "pip install torchtext spacy\n",
    "python -m spacy download en\n",
    "python -m spacy download de\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWRjzeb2qMlU"
   },
   "source": [
    "## Data Loading\n",
    "torchtext를 이용하여 Multi30k 데이터셋을 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "executionInfo": {
     "elapsed": 4140,
     "status": "error",
     "timestamp": 1609046130131,
     "user": {
      "displayName": "Seong-Hwan Heo",
      "photoUrl": "https://lh5.googleusercontent.com/-0AkRh-q73zM/AAAAAAAAAAI/AAAAAAAAL_I/ftyesh65uHs/s64/photo.jpg",
      "userId": "17008924282569558701"
     },
     "user_tz": -540
    },
    "id": "hnBIxOwlqMlU",
    "outputId": "ad5a4918-b98e-4cb5-dee4-b4b1506049dd"
   },
   "outputs": [],
   "source": [
    "# For data loading.\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "SRC = Field(tokenize = 'spacy',\n",
    "            tokenizer_language='de',\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True)\n",
    "\n",
    "TRG = Field(tokenize='spacy',\n",
    "            tokenizer_language='en',\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'),\n",
    "                                                    fields=(SRC, TRG))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsJdgrqjqMlV"
   },
   "source": [
    "## BucketIterator\n",
    "다음은 torchtext에서 제공하는 BucketIterator를 사용하여 데이터셋을 batch 단위로 나누고, iterator를 생성한다. for문과 같은 반복문을 통해 각 batch에 순차적으로 접근할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hEy2WxyHqMlV"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    return Batch(src, trg,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로, 모델 및 optimizer를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TRG.vocab.stoi[\"<blank>\"]\n",
    "model = make_model(len(SRC.vocab), len(TRG.vocab), N=6)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# GPU를 통해 병렬 처리 가능\n",
    "model_par = nn.DataParallel(model, device_ids=[device])\n",
    "\n",
    "optim = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model_par.train()\n",
    "    run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
    "                model_par, \n",
    "                SimpleLossCompute(model.generator, crit , opt=optim))\n",
    "    model_par.eval()\n",
    "    loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
    "                        model_par, \n",
    "                        SimpleLossCompute(model.generator, crit, opt=None))\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 학습시킨 모델이 어떤 결과를 내는지 <b>greedy decoding</b>을 통해 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           ys, \n",
    "                           subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "for i, batch in enumerate(valid_iter):\n",
    "    src = batch.src.transpose(0, 1)[:1]\n",
    "    print(\"\\n    원문:\", end='\\t')\n",
    "    for j in range(1, batch.src.size(0)):\n",
    "        sym = SRC.vocab.itos[batch.src.data[j, 0]]\n",
    "        if sym == \"<eos>\": break\n",
    "        print(sym, end =\" \")\n",
    "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
    "    out = greedy_decode(model, src, src_mask, \n",
    "                        max_len=60, start_symbol=TRG.vocab.stoi[\"<sos>\"])\n",
    "    print(\"\\n기계번역:\", end=\"\\t\")\n",
    "    for j in range(1, out.size(1)):\n",
    "        sym = TRG.vocab.itos[out[0, j]]\n",
    "        if sym == \"<eos>\": break\n",
    "        print(sym, end =\" \")\n",
    "    print(\"\\n실제번역:\", end=\"\\t\")\n",
    "    for j in range(1, batch.trg.size(0)):\n",
    "        sym = TRG.vocab.itos[batch.trg.data[j, 0]]\n",
    "        if sym == \"<eos>\": break\n",
    "        print(sym, end =\" \")\n",
    "        \n",
    "    if i == 2: # 3개까지만 출력\n",
    "        break\n",
    "    \n",
    "    print(\"=\"*20)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wsOCJD5gqMk9",
    "ktHD0TQeqMlO",
    "5exC6DPeqMlW",
    "sDBp0vunqMlX",
    "LVl77sKBqMlg"
   ],
   "name": "The Annotated Transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
